"use strict";

function asyncGeneratorStep(gen, resolve, reject, _next, _throw, key, arg) { try { var info = gen[key](arg); var value = info.value; } catch (error) { reject(error); return; } if (info.done) { resolve(value); } else { Promise.resolve(value).then(_next, _throw); } }
function _asyncToGenerator(fn) { return function () { var self = this, args = arguments; return new Promise(function (resolve, reject) { var gen = fn.apply(self, args); function _next(value) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, "next", value); } function _throw(err) { asyncGeneratorStep(gen, resolve, reject, _next, _throw, "throw", err); } _next(undefined); }); }; }
<<<<<<< Updated upstream
var os = require('os');
var path = require('path');
var zlib = require('zlib');
var fse = require('fs-extra');
var miss = require('mississippi');
var split = require('split2');
var archiver = require('archiver');
var debug = require('./debug');
var AssetHandler = require('./AssetHandler');
var stringifyStream = require('./stringifyStream');
var validateOptions = require('./validateOptions');
var rejectOnApiError = require('./rejectOnApiError');
var getDocumentsStream = require('./getDocumentsStream');
var filterSystemDocuments = require('./filterSystemDocuments');
var filterDocumentTypes = require('./filterDocumentTypes');
var filterDrafts = require('./filterDrafts');
var logFirstChunk = require('./logFirstChunk');
var tryParseJson = require('./tryParseJson');
var noop = () => null;
=======

const os = require('os');

const path = require('path');

const zlib = require('zlib');

const fse = require('fs-extra');

const miss = require('mississippi');

const split = require('split2');

const archiver = require('archiver');

const debug = require('./debug');

const AssetHandler = require('./AssetHandler');

const stringifyStream = require('./stringifyStream');

const validateOptions = require('./validateOptions');

const rejectOnApiError = require('./rejectOnApiError');

const getDocumentsStream = require('./getDocumentsStream');

const filterSystemDocuments = require('./filterSystemDocuments');

const filterDocumentTypes = require('./filterDocumentTypes');

const filterDrafts = require('./filterDrafts');

const noop = () => null;

>>>>>>> Stashed changes
function exportDataset(opts) {
  const options = validateOptions(opts);
  const onProgress = options.onProgress || noop;
  const archive = archiver('tar', {
    gzip: true,
    gzipOptions: {
      level: options.compress ? zlib.Z_DEFAULT_COMPRESSION : zlib.Z_NO_COMPRESSION
    }
  });
<<<<<<< Updated upstream
  var slugDate = new Date().toISOString().replace(/[^a-z0-9]/gi, '-').toLowerCase();
  var prefix = "".concat(opts.dataset, "-export-").concat(slugDate);
  var tmpDir = path.join(os.tmpdir(), prefix);
  var cleanup = () => fse.remove(tmpDir).catch(err => {
    debug("Error while cleaning up temporary files: ".concat(err.message));
  });
  var assetHandler = new AssetHandler({
=======
  const slugDate = new Date().toISOString().replace(/[^a-z0-9]/gi, '-').toLowerCase();
  const prefix = `${opts.dataset}-export-${slugDate}`;
  const tmpDir = path.join(os.tmpdir(), prefix);
  const assetHandler = new AssetHandler({
>>>>>>> Stashed changes
    client: options.client,
    tmpDir,
    prefix
  });
  debug('Outputting assets (temporarily) to %s', tmpDir);
  debug('Outputting to %s', options.outputPath === '-' ? 'stdout' : options.outputPath);
<<<<<<< Updated upstream
  var outputStream;
  if (isWritableStream(options.outputPath)) {
    outputStream = options.outputPath;
  } else {
    outputStream = options.outputPath === '-' ? process.stdout : fse.createWriteStream(options.outputPath);
  }
  var assetStreamHandler = assetHandler.noop;
  if (!options.raw) {
    assetStreamHandler = options.assets ? assetHandler.rewriteAssets : assetHandler.stripAssets;
  }
  return new Promise( /*#__PURE__*/function () {
    var _ref = _asyncToGenerator(function* (resolve, reject) {
      miss.finished(archive, /*#__PURE__*/function () {
        var _ref2 = _asyncToGenerator(function* (archiveErr) {
          if (archiveErr) {
            debug('Archiving errored! %s', archiveErr.stack);
            yield cleanup();
            reject(archiveErr);
            return;
          }
          debug('Archive finished!');
        });
        return function (_x3) {
          return _ref2.apply(this, arguments);
        };
      }());
=======
  const outputStream = options.outputPath === '-' ? process.stdout : fse.createWriteStream(options.outputPath);
  let assetStreamHandler = assetHandler.noop;

  if (!options.raw) {
    assetStreamHandler = options.assets ? assetHandler.rewriteAssets : assetHandler.stripAssets;
  }

  return new Promise(
  /*#__PURE__*/
  function () {
    var _ref = _asyncToGenerator(function* (resolve, reject) {
      miss.finished(archive, archiveErr => {
        if (archiveErr) {
          debug('Archiving errored! %s', archiveErr.stack);
          reject(archiveErr);
          return;
        }

        debug('Archive finished!');
      });
>>>>>>> Stashed changes
      debug('Getting dataset export stream');
      onProgress({
        step: 'Exporting documents...'
      });
<<<<<<< Updated upstream
      var documentCount = 0;
      var lastReported = Date.now();
      var reportDocumentCount = (chunk, enc, cb) => {
        ++documentCount;
        var now = Date.now();
=======
      let documentCount = 0;
      let lastReported = Date.now();

      const reportDocumentCount = (chunk, enc, cb) => {
        ++documentCount;
        const now = Date.now();

>>>>>>> Stashed changes
        if (now - lastReported > 50) {
          onProgress({
            step: 'Exporting documents...',
            current: documentCount,
            total: '?',
            update: true
          });
          lastReported = now;
        }
        cb(null, chunk);
      };
<<<<<<< Updated upstream
      var inputStream = yield getDocumentsStream(options.client, options.dataset);
      debug('Got HTTP %d', inputStream.statusCode);
      debug('Response headers: %o', inputStream.headers);
      var jsonStream = miss.pipeline(inputStream, logFirstChunk(), split(tryParseJson), rejectOnApiError(), filterSystemDocuments(), assetStreamHandler, filterDocumentTypes(options.types), options.drafts ? miss.through.obj() : filterDrafts(), stringifyStream(), miss.through(reportDocumentCount));
      miss.finished(jsonStream, /*#__PURE__*/function () {
        var _ref3 = _asyncToGenerator(function* (err) {
=======

      const inputStream = yield getDocumentsStream(options.client, options.dataset);
      const jsonStream = miss.pipeline(inputStream, split(JSON.parse), rejectOnApiError, filterSystemDocuments, assetStreamHandler, filterDocumentTypes(options.types), options.drafts ? miss.through.obj() : filterDrafts, stringifyStream, miss.through(reportDocumentCount));
      miss.finished(jsonStream,
      /*#__PURE__*/
      function () {
        var _ref2 = _asyncToGenerator(function* (err) {
>>>>>>> Stashed changes
          if (err) {
            return;
          }
          onProgress({
            step: 'Exporting documents...',
            current: documentCount,
            total: documentCount,
            update: true
          });
          if (!options.raw && options.assets) {
            onProgress({
              step: 'Downloading assets...'
            });
          }
<<<<<<< Updated upstream
          var prevCompleted = 0;
          var progressInterval = setInterval(() => {
            var completed = assetHandler.queueSize - assetHandler.queue.size - assetHandler.queue.pending;
=======

          let prevCompleted = 0;
          const progressInterval = setInterval(() => {
            const completed = assetHandler.queueSize - assetHandler.queue.size;

>>>>>>> Stashed changes
            if (prevCompleted === completed) {
              return;
            }
            prevCompleted = completed;
            onProgress({
              step: 'Downloading assets...',
              current: completed,
              total: assetHandler.queueSize,
              update: true
            });
          }, 500);
          debug('Waiting for asset handler to complete downloads');
          try {
<<<<<<< Updated upstream
            var assetMap = yield assetHandler.finish();

            // Make sure we mark the progress as done (eg 100/100 instead of 99/100)
            onProgress({
              step: 'Downloading assets...',
              current: assetHandler.queueSize,
              total: assetHandler.queueSize,
              update: true
            });
=======
            const assetMap = yield assetHandler.finish();
>>>>>>> Stashed changes
            archive.append(JSON.stringify(assetMap), {
              name: 'assets.json',
              prefix
            });
            clearInterval(progressInterval);
          } catch (assetErr) {
            clearInterval(progressInterval);
            reject(assetErr);
            return;
          }

<<<<<<< Updated upstream
          // Add all downloaded assets to archive
          archive.directory(path.join(tmpDir, 'files'), "".concat(prefix, "/files"), {
=======
          archive.directory(path.join(tmpDir, 'files'), `${prefix}/files`, {
>>>>>>> Stashed changes
            store: true
          });
          archive.directory(path.join(tmpDir, 'images'), `${prefix}/images`, {
            store: true
          });
          debug('Finalizing archive, flushing streams');
          onProgress({
            step: 'Adding assets to archive...'
          });
          archive.finalize();
        });
<<<<<<< Updated upstream
        return function (_x4) {
          return _ref3.apply(this, arguments);
=======

        return function (_x3) {
          return _ref2.apply(this, arguments);
>>>>>>> Stashed changes
        };
      }());
      archive.on('warning', err => {
        debug('Archive warning: %s', err.message);
      });
      archive.append(jsonStream, {
        name: 'data.ndjson',
        prefix
      });
      miss.pipe(archive, outputStream, onComplete);
<<<<<<< Updated upstream
      function onComplete(_x5) {
=======

      function onComplete(_x4) {
>>>>>>> Stashed changes
        return _onComplete.apply(this, arguments);
      }
      function _onComplete() {
        _onComplete = _asyncToGenerator(function* (err) {
          onProgress({
            step: 'Clearing temporary files...'
          });
<<<<<<< Updated upstream
          yield cleanup();
=======
          yield fse.remove(tmpDir);

>>>>>>> Stashed changes
          if (!err) {
            resolve();
            return;
          }
          debug('Error during streaming: %s', err.stack);
          assetHandler.clear();
          reject(err);
        });
        return _onComplete.apply(this, arguments);
      }
    });
    return function (_x, _x2) {
      return _ref.apply(this, arguments);
    };
  }());
}
<<<<<<< Updated upstream
function isWritableStream(val) {
  return val !== null && typeof val === 'object' && typeof val.pipe === 'function' && typeof val._write === 'function' && typeof val._writableState === 'object';
}
=======

>>>>>>> Stashed changes
module.exports = exportDataset;